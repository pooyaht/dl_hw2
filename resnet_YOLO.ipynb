{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e52fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab - Setting up environment...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    repo_url = \"https://github.com/pooyaht/dl_hw2\"\n",
    "    clone_path = \"/content/drive/MyDrive/dl_hw2\"\n",
    "    \n",
    "    if not os.path.exists(clone_path):\n",
    "        print(f\"Cloning repository to {clone_path}...\")\n",
    "        !git clone {repo_url} {clone_path}\n",
    "    else:\n",
    "        print(f\"Repository already exists at {clone_path}\")\n",
    "    \n",
    "    os.chdir(clone_path)\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    \n",
    "    if clone_path not in sys.path:\n",
    "        sys.path.append(clone_path)\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from coco_downloader import COCOCatDogDownloader\n",
    "from dataloader import create_split_datasets, create_dataloaders\n",
    "from yolo_helpers import create_yolo_targets, decode_yolo_predictions, yolo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1228a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations already exist, skipping download\n",
      "loading annotations into memory...\n",
      "Done (t=9.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Cat category ID: [17]\n",
      "Dog category ID: [18]\n",
      "Found 4078 pure cat images\n",
      "Found 4342 pure dog images\n",
      "Found 220 mixed images\n",
      "Selected 220 mixed images (all)\n",
      "Selected 2500 pure cat images\n",
      "Selected 2500 pure dog images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5220/5220 [00:00<00:00, 92811.64it/s]\n"
     ]
    }
   ],
   "source": [
    "downloader = COCOCatDogDownloader()\n",
    "downloader.download_and_prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874afd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Results:\n",
      "Training set: 4437 images\n",
      "- Mixed: 187\n",
      "- Pure cat: 2125\n",
      "- Pure dog: 2125\n",
      "Validation set: 783 images\n",
      "- Mixed: 33\n",
      "- Pure cat: 375\n",
      "- Pure dog: 375\n",
      "\n",
      "Saved:\n",
      "- Training annotations: cat_dog_images/train_annotations.json\n",
      "- Validation annotations: cat_dog_images/val_annotations.json\n"
     ]
    }
   ],
   "source": [
    "annotations_file = \"cat_dog_images/cat_dog_annotations.json\"\n",
    "\n",
    "train_file, val_file = create_split_datasets(\n",
    "    annotations_file, val_ratio=0.15)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_file, val_file,\n",
    "    images_dir=\"cat_dog_images\",  \n",
    "    batch_size=16,\n",
    "    target_size=(224, 224)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13c01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07443423,  2.33441195],\n",
       "       [ 5.45975762,  6.25354672],\n",
       "       [ 9.75938853, 10.51438995]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_anchors(train_loader, num_anchors=3, grid_size=14):\n",
    "    all_boxes = []\n",
    "    for _, targets in train_loader:\n",
    "        for target in targets:\n",
    "            for box in target['boxes']:\n",
    "                w = (box[2] - box[0]) * grid_size / 224\n",
    "                h = (box[3] - box[1]) * grid_size / 224\n",
    "                all_boxes.append([w.item(), h.item()])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_anchors, random_state=42)\n",
    "    kmeans.fit(all_boxes)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "anchors = calculate_anchors(train_loader)\n",
    "anchors = np.sort(anchors, axis=0)\n",
    "anchors = anchors.astype(np.float32)\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFeatureAdapter(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout_rate=0.1):\n",
    "        super(ResidualFeatureAdapter, self).__init__()\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(hidden_channels, in_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.adapter(x)\n",
    "    \n",
    "class ResNetYOLODetector(nn.Module):\n",
    "    def __init__(self, anchor_boxes, backbone_name=\"resnet50\", grid_size=14, freeze_backbone_epochs=15, dropout_rate=0.1):\n",
    "        super(ResNetYOLODetector, self).__init__()\n",
    "    \n",
    "        self.num_classes = 2\n",
    "        self.num_anchors = len(anchor_boxes)\n",
    "        self.freeze_backbone_epochs = freeze_backbone_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.backbone, backbone_channels = self._load_backbone(backbone_name, grid_size)\n",
    "        self.feature_adapter = ResidualFeatureAdapter(backbone_channels, backbone_channels // 2, dropout_rate)\n",
    "        \n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Conv2d(backbone_channels, backbone_channels // 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(backbone_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(backbone_channels // 4, self.num_anchors * (5 + self.num_classes), kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('anchors', torch.tensor(anchor_boxes) \\\n",
    "                              if not isinstance(anchor_boxes, torch.Tensor) else anchor_boxes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        final_conv = self.prediction_head[-1]\n",
    "        num_outputs_per_anchor = 5 + self.num_classes\n",
    "        for i in range(self.num_anchors):\n",
    "            obj_idx = i * num_outputs_per_anchor + 4\n",
    "            nn.init.constant_(final_conv.bias[obj_idx], -np.log((1 - 0.01) / 0.01))\n",
    "     \n",
    "        self.freeze_backbone()\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze_backbone_layers(self, num_layers=2):\n",
    "        backbone_layers = list(self.backbone.children())\n",
    "        \n",
    "        if num_layers > 0:\n",
    "            layers_to_unfreeze = backbone_layers[-num_layers:]\n",
    "            for layer in layers_to_unfreeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "    \n",
    "    def update_epoch(self, epoch, partial_unfreeze_layers=2):\n",
    "        self.current_epoch = epoch\n",
    "        if epoch >= self.freeze_backbone_epochs and self.backbone[0].weight.requires_grad == False:\n",
    "            if partial_unfreeze_layers == -1:\n",
    "                self.unfreeze_backbone()\n",
    "                print(f\"Epoch {epoch}: Unfroze entire backbone\")\n",
    "            else:\n",
    "                self.unfreeze_backbone_layers(partial_unfreeze_layers) \n",
    "                print(f\"Epoch {epoch}: Unfroze last {partial_unfreeze_layers} backbone layers\")\n",
    "\n",
    "    def _load_backbone(self, backbone_name, grid_size=7):\n",
    "   \n",
    "        backbone = torch.hub.load('pytorch/vision:v0.10.0', backbone_name, pretrained=True)\n",
    "        \n",
    "        if backbone_name in ['resnet18', 'resnet34']:\n",
    "            final_channels = 512\n",
    "            has_bottleneck = False\n",
    "        else:\n",
    "            final_channels = 2048\n",
    "            has_bottleneck = True\n",
    "        \n",
    "        if grid_size == 7:\n",
    "            backbone_modified = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            output_channels = final_channels\n",
    "            \n",
    "        elif grid_size == 14:\n",
    "            if has_bottleneck:\n",
    "                backbone.layer4[0].conv2.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            else:\n",
    "                backbone.layer4[0].conv1.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            \n",
    "            backbone_modified = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            output_channels = final_channels\n",
    "            \n",
    "        elif grid_size == 28:\n",
    "            if has_bottleneck:\n",
    "                backbone.layer3[0].conv2.stride = (1, 1)\n",
    "                backbone.layer3[0].downsample[0].stride = (1, 1)\n",
    "                backbone.layer4[0].conv2.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            else:\n",
    "                backbone.layer3[0].conv1.stride = (1, 1)\n",
    "                backbone.layer3[0].downsample[0].stride = (1, 1)\n",
    "                backbone.layer4[0].conv1.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            \n",
    "            backbone_modified = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            output_channels = final_channels\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported grid size: {grid_size}. Supported: [7, 14, 28]\")\n",
    "        \n",
    "        print(f\"Backbone {backbone_name} configured for {grid_size}x{grid_size} grid\")\n",
    "        print(f\"Output channels: {output_channels}\")\n",
    "        print(f\"Approximate backbone parameters: {sum(p.numel() for p in backbone_modified.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return backbone_modified, output_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        adapted_features = self.feature_adapter(features)\n",
    "        predictions = self.prediction_head(adapted_features)\n",
    "        \n",
    "        predictions = predictions.view(\n",
    "            batch_size,\n",
    "            self.num_anchors,\n",
    "            5 + self.num_classes,\n",
    "            predictions.size(-2),\n",
    "            predictions.size(-1)\n",
    "        )\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        backbone_params = sum(p.numel() for p in self.backbone.parameters())\n",
    "        adapter_params = sum(p.numel() for p in self.feature_adapter.parameters())\n",
    "        head_params = sum(p.numel() for p in self.prediction_head.parameters())\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'backbone_parameters': backbone_params,\n",
    "            'adapter_parameters': adapter_params,\n",
    "            'head_parameters': head_params,\n",
    "            'backbone_frozen': not self.backbone[0].weight.requires_grad,\n",
    "            'current_epoch': self.current_epoch\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b42bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, anchors, train_loader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    model.update_epoch(epoch, partial_unfreeze_layers=2)\n",
    "    \n",
    "    running_losses = {'total': 0.0, 'xy': 0.0, 'wh': 0.0, 'obj': 0.0, 'noobj': 0.0, 'cls': 0.0}\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(pbar):\n",
    "        images = torch.stack(images).to(device)\n",
    "        \n",
    "        yolo_targets = create_yolo_targets(\n",
    "            targets, \n",
    "            anchors\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        losses = yolo_loss(outputs, yolo_targets)\n",
    "        losses['total_loss'].backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        for key in running_losses:\n",
    "            if key == 'total':\n",
    "                running_losses[key] += losses['total_loss'].item()\n",
    "            else:\n",
    "                loss_key = f'{key}_loss'\n",
    "                if loss_key in losses:\n",
    "                    running_losses[key] += losses[loss_key].item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = running_losses['total'] / (batch_idx + 1)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{current_lr:.2e}'})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    num_batches = len(train_loader)\n",
    "    avg_losses = {key: running_losses[key] / num_batches for key in running_losses}\n",
    "    \n",
    "    return avg_losses\n",
    "\n",
    "def validate_epoch(model, anchors, val_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    running_losses = {'total': 0.0, 'xy': 0.0, 'wh': 0.0, 'obj': 0.0, 'noobj': 0.0, 'cls': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc='Validation'):\n",
    "            images = torch.stack(images).to(device)\n",
    "            \n",
    "            yolo_targets = create_yolo_targets(\n",
    "                targets, \n",
    "                anchors\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            losses = yolo_loss(outputs, yolo_targets)\n",
    "            \n",
    "            for key in running_losses:\n",
    "                if key == 'total':\n",
    "                    running_losses[key] += losses['total_loss'].item()\n",
    "                else:\n",
    "                    loss_key = f'{key}_loss'\n",
    "                    if loss_key in losses:\n",
    "                        running_losses[key] += losses[loss_key].item()\n",
    "    \n",
    "    num_batches = len(val_loader)\n",
    "    avg_losses = {key: running_losses[key] / num_batches for key in running_losses}\n",
    "    \n",
    "    return avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2afe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss\n",
    "\n",
    "def plot_losses(train_losses, val_losses, save_path=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, len(train_losses['total']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Training and Validation Losses')\n",
    "    \n",
    "    loss_types = ['total', 'xy', 'wh', 'obj', 'noobj', 'cls']\n",
    "    titles = ['Total Loss', 'XY Loss', 'WH Loss', 'Objectness Loss', 'No-Object Loss', 'Classification Loss']\n",
    "    \n",
    "    for i, (loss_type, title) in enumerate(zip(loss_types, titles)):\n",
    "        ax = axes[i//3, i%3]\n",
    "        ax.plot(epochs, train_losses[loss_type], 'b-', label='Train')\n",
    "        ax.plot(epochs, val_losses[loss_type], 'r-', label='Validation')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(f\"CUDA available: Using GPU - {torch.cuda.get_device_name()}\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"MPS available: Using Apple Silicon GPU\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "def main():\n",
    "    from pathlib import Path\n",
    "    \n",
    "    config = {\n",
    "        'experiment_name': 'resnet50_YOLO_improved_regularization_grid_size_14',\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 25,\n",
    "        'freeze_backbone_epochs': 20,\n",
    "        'device': get_device(),\n",
    "        'target_size': (224, 224),\n",
    "        'lr': 3e-4,  \n",
    "        'weight_decay': 1e-2,  \n",
    "        'dropout_rate': 0.15,\n",
    "    }\n",
    "    \n",
    "    print(f\"Using device: {config['device']}\")\n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    save_dir = \"checkpoints\" / Path(config['experiment_name'])\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    model = ResNetYOLODetector(\n",
    "        anchor_boxes=anchors, \n",
    "        backbone_name=\"resnet50\", \n",
    "        freeze_backbone_epochs=config['freeze_backbone_epochs'],\n",
    "        grid_size=14,\n",
    "        dropout_rate=config['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    info = model.get_model_info()\n",
    "    print(\"Model Information:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['lr'], \n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=config['num_epochs'],\n",
    "        eta_min=config['lr'] * 0.01\n",
    "    )\n",
    "    \n",
    "    train_history = {'total': [], 'xy': [], 'wh': [], 'obj': [], 'noobj': [], 'cls': []}\n",
    "    val_history = {'total': [], 'xy': [], 'wh': [], 'obj': [], 'noobj': [], 'cls': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_losses = train_epoch(model, anchors, train_loader, optimizer, scheduler, device, epoch)\n",
    "        val_losses = validate_epoch(model, anchors, val_loader, device)\n",
    "        \n",
    "        print(f\"Train - Total: {train_losses['total']:.4f}, \"\n",
    "              f\"XY: {train_losses['xy']:.4f}, \"\n",
    "              f\"WH: {train_losses['wh']:.4f}, \"\n",
    "              f\"Obj: {train_losses['obj']:.4f}, \"\n",
    "              f\"NoObj: {train_losses['noobj']:.4f}, \"\n",
    "              f\"Cls: {train_losses['cls']:.4f}\")\n",
    "        \n",
    "        print(f\"Val   - Total: {val_losses['total']:.4f}, \"\n",
    "              f\"XY: {val_losses['xy']:.4f}, \"\n",
    "              f\"WH: {val_losses['wh']:.4f}, \"\n",
    "              f\"Obj: {val_losses['obj']:.4f}, \"\n",
    "              f\"NoObj: {val_losses['noobj']:.4f}, \"\n",
    "              f\"Cls: {val_losses['cls']:.4f}\")\n",
    "        \n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        for key in train_history:\n",
    "            train_history[key].append(train_losses[key])\n",
    "            val_history[key].append(val_losses[key])\n",
    "        \n",
    "        if val_losses['total'] < best_val_loss:\n",
    "            best_val_loss = val_losses['total']\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, val_losses['total'],\n",
    "                save_dir / 'best_model.pth'\n",
    "            )\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    plot_losses(train_history, val_history, \n",
    "                save_path=save_dir / 'training_curves.png')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjl2ba4z7fk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_inference(model_path, device='cpu'):\n",
    "    dummy_anchors = torch.tensor([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]])  \n",
    "    model = ResNetYOLODetector(\n",
    "        anchor_boxes=dummy_anchors,\n",
    "        backbone_name=\"resnet50\", \n",
    "        freeze_backbone_epochs=12,\n",
    "        grid_size=14\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "    print(f\"Model was trained for {checkpoint['epoch']} epochs\")\n",
    "    print(f\"Best validation loss: {checkpoint['loss']:.4f}\")\n",
    "    print(f\"Loaded anchors from model: {model.anchors}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def inference_on_images(model, image_paths, device='cpu', conf_threshold=0.5, target_size=(224, 224)):\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    anchors = model.anchors\n",
    "    \n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    class_names = {0: 'dog', 1: 'cat'}\n",
    "    colors = {0: 'red', 1: 'blue'}\n",
    "    \n",
    "    num_images = len(image_paths)\n",
    "    cols = min(3, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    _, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, image_path in enumerate(image_paths):\n",
    "            original_image = Image.open(image_path).convert('RGB')\n",
    "            original_size = original_image.size\n",
    "            \n",
    "            input_tensor = transform(original_image).unsqueeze(0).to(device)\n",
    "            \n",
    "            predictions = model(input_tensor)\n",
    "            \n",
    "            detections = decode_yolo_predictions(predictions, anchors, conf_threshold=conf_threshold)\n",
    "            \n",
    "            scale_x = original_size[0] / target_size[0]\n",
    "            scale_y = original_size[1] / target_size[1]\n",
    "            \n",
    "            draw_image = original_image.copy()\n",
    "            draw = ImageDraw.Draw(draw_image)\n",
    "            \n",
    "            detection_count = {'cat': 0, 'dog': 0}\n",
    "            \n",
    "            if len(detections) > 0 and len(detections[0]) > 0:\n",
    "                boxes = detections[0]['boxes']\n",
    "                scores = detections[0]['scores']\n",
    "                labels = detections[0]['labels']\n",
    "\n",
    "                for i in range(len(boxes)):\n",
    "                    x1, y1, x2, y2 = boxes[i]\n",
    "                    conf = scores[i]\n",
    "                    cls = labels[i]\n",
    "                    \n",
    "                    x1 = int(x1 * scale_x)\n",
    "                    y1 = int(y1 * scale_y)\n",
    "                    x2 = int(x2 * scale_x)\n",
    "                    y2 = int(y2 * scale_y)\n",
    "                    \n",
    "                    class_id = int(cls)\n",
    "                    class_name = class_names[class_id]\n",
    "                    color = colors[class_id]\n",
    "                    detection_count[class_name] += 1\n",
    " \n",
    "                    draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "                    \n",
    "                    label = f'{class_name}: {conf:.2f}'\n",
    "                    \n",
    "                    try:\n",
    "                        font = ImageFont.truetype(\"/System/Library/Fonts/Arial.ttf\", 16)\n",
    "                    except:\n",
    "                        font = ImageFont.load_default()\n",
    "                    \n",
    "                    bbox = draw.textbbox((0, 0), label, font=font)\n",
    "                    text_width = bbox[2] - bbox[0]\n",
    "                    text_height = bbox[3] - bbox[1]\n",
    "                    \n",
    "                    draw.rectangle([x1, y1-text_height-4, x1+text_width+4, y1], fill=color)\n",
    "                    draw.text((x1+2, y1-text_height-2), label, fill='white', font=font)\n",
    "            \n",
    "            ax = axes[idx] if num_images > 1 else axes[0]\n",
    "            ax.imshow(draw_image)\n",
    "            ax.set_title(f'Image {idx+1}\\nCats: {detection_count[\"cat\"]}, Dogs: {detection_count[\"dog\"]}')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            print(f\"Image {idx+1}: {image_path}\")\n",
    "            print(f\"  Detections: {len(detections[0]) if len(detections) > 0 else 0}\")\n",
    "            print(f\"  Cats: {detection_count['cat']}, Dogs: {detection_count['dog']}\")\n",
    "    \n",
    "    for idx in range(num_images, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load your trained model (anchors automatically loaded from model)\n",
    "model_path = \"checkpoints/resnet50_YOLO_residual_adapter_14/best_model.pth\"\n",
    "device = get_device()\n",
    "\n",
    "# Load the model - anchors are now loaded automatically from the saved model\n",
    "# model = load_model_for_inference(model_path, device)\n",
    "\n",
    "# Test on single image\n",
    "# inference_on_images(model, \"path/to/your/test/image.jpg\", device)\n",
    "\n",
    "# Test on multiple images\n",
    "# image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", \"path/to/image3.jpg\"]\n",
    "# inference_on_images(model, image_paths, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
