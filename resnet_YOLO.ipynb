{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8a5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from coco_downloader import COCOCatDogDownloader\n",
    "from dataloader import create_split_datasets, create_dataloaders\n",
    "from yolo_helpers import create_yolo_targets, decode_yolo_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1228a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations already exist, skipping download\n",
      "loading annotations into memory...\n",
      "Done (t=9.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Cat category ID: [17]\n",
      "Dog category ID: [18]\n",
      "Found 4078 pure cat images\n",
      "Found 4342 pure dog images\n",
      "Found 220 mixed images\n",
      "Selected 220 mixed images (all)\n",
      "Selected 2500 pure cat images\n",
      "Selected 2500 pure dog images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5220/5220 [00:00<00:00, 92811.64it/s]\n"
     ]
    }
   ],
   "source": [
    "downloader = COCOCatDogDownloader()\n",
    "downloader.download_and_prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874afd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Results:\n",
      "Training set: 4437 images\n",
      "- Mixed: 187\n",
      "- Pure cat: 2125\n",
      "- Pure dog: 2125\n",
      "Validation set: 783 images\n",
      "- Mixed: 33\n",
      "- Pure cat: 375\n",
      "- Pure dog: 375\n",
      "\n",
      "Saved:\n",
      "- Training annotations: cat_dog_images/train_annotations.json\n",
      "- Validation annotations: cat_dog_images/val_annotations.json\n"
     ]
    }
   ],
   "source": [
    "annotations_file = \"cat_dog_images/cat_dog_annotations.json\"\n",
    "\n",
    "train_file, val_file = create_split_datasets(\n",
    "    annotations_file, val_ratio=0.15)\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_file, val_file,\n",
    "    images_dir=\"cat_dog_images\",  \n",
    "    batch_size=16,\n",
    "    target_size=(224, 224)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13c01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07443423,  2.33441195],\n",
       "       [ 5.45975762,  6.25354672],\n",
       "       [ 9.75938853, 10.51438995]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_anchors(train_loader, num_anchors=3, grid_size=14):\n",
    "    all_boxes = []\n",
    "    for _, targets in train_loader:\n",
    "        for target in targets:\n",
    "            for box in target['boxes']:\n",
    "                w = (box[2] - box[0]) * grid_size / 224\n",
    "                h = (box[3] - box[1]) * grid_size / 224\n",
    "                all_boxes.append([w.item(), h.item()])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_anchors, random_state=42)\n",
    "    kmeans.fit(all_boxes)\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "anchors = calculate_anchors(train_loader)\n",
    "anchors = np.sort(anchors, axis=0)\n",
    "anchors = anchors.astype(np.float32)\n",
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f35aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualFeatureAdapter(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout_rate=0.1):\n",
    "        super(ResidualFeatureAdapter, self).__init__()\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(hidden_channels, in_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.adapter(x)\n",
    "    \n",
    "class ResNetYOLODetector(nn.Module):\n",
    "    def __init__(self, anchor_boxes, backbone_name=\"resnet50\", grid_size=14, freeze_backbone_epochs=15, dropout_rate=0.1):\n",
    "        super(ResNetYOLODetector, self).__init__()\n",
    "    \n",
    "        self.num_classes = 2\n",
    "        self.num_anchors = len(anchor_boxes)\n",
    "        self.freeze_backbone_epochs = freeze_backbone_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.backbone, backbone_channels = self._load_backbone(backbone_name, grid_size)\n",
    "        self.feature_adapter = ResidualFeatureAdapter(backbone_channels, backbone_channels // 2, dropout_rate)\n",
    "        \n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Conv2d(backbone_channels, backbone_channels // 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(backbone_channels // 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(backbone_channels // 4, self.num_anchors * (5 + self.num_classes), kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('anchors', torch.tensor(anchor_boxes) \\\n",
    "                              if not isinstance(anchor_boxes, torch.Tensor) else anchor_boxes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        final_conv = self.prediction_head[-1]\n",
    "        num_outputs_per_anchor = 5 + self.num_classes\n",
    "        for i in range(self.num_anchors):\n",
    "            obj_idx = i * num_outputs_per_anchor + 4\n",
    "            nn.init.constant_(final_conv.bias[obj_idx], -np.log((1 - 0.01) / 0.01))\n",
    "     \n",
    "        self.freeze_backbone()\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def unfreeze_backbone_layers(self, num_layers=2):\n",
    "        backbone_layers = list(self.backbone.children())\n",
    "        \n",
    "        if num_layers > 0:\n",
    "            layers_to_unfreeze = backbone_layers[-num_layers:]\n",
    "            for layer in layers_to_unfreeze:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "    \n",
    "    def update_epoch(self, epoch, partial_unfreeze_layers=2):\n",
    "        self.current_epoch = epoch\n",
    "        if epoch >= self.freeze_backbone_epochs and self.backbone[0].weight.requires_grad == False:\n",
    "            if partial_unfreeze_layers == -1:\n",
    "                self.unfreeze_backbone()\n",
    "                print(f\"ðŸ”“ Epoch {epoch}: Unfroze entire backbone\")\n",
    "            else:\n",
    "                self.unfreeze_backbone_layers(partial_unfreeze_layers) \n",
    "                print(f\"ðŸ”“ Epoch {epoch}: Unfroze last {partial_unfreeze_layers} backbone layers\")\n",
    "\n",
    "    def _load_backbone(self, backbone_name, grid_size=7):\n",
    "   \n",
    "        backbone = torch.hub.load('pytorch/vision:v0.10.0', backbone_name, pretrained=True)\n",
    "        \n",
    "        if backbone_name in ['resnet18', 'resnet34']:\n",
    "            final_channels = 512\n",
    "            has_bottleneck = False\n",
    "        else:\n",
    "            final_channels = 2048\n",
    "            has_bottleneck = True\n",
    "        \n",
    "        if grid_size == 7:\n",
    "            backbone_modified = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            output_channels = final_channels\n",
    "            \n",
    "        elif grid_size == 14:\n",
    "            if has_bottleneck:\n",
    "                backbone.layer4[0].conv2.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            else:\n",
    "                backbone.layer4[0].conv1.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            \n",
    "            backbone_modified = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            output_channels = final_channels\n",
    "            \n",
    "        elif grid_size == 28:\n",
    "            if has_bottleneck:\n",
    "                backbone.layer3[0].conv2.stride = (1, 1)\n",
    "                backbone.layer3[0].downsample[0].stride = (1, 1)\n",
    "                backbone.layer4[0].conv2.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            else:\n",
    "                backbone.layer3[0].conv1.stride = (1, 1)\n",
    "                backbone.layer3[0].downsample[0].stride = (1, 1)\n",
    "                backbone.layer4[0].conv1.stride = (1, 1)\n",
    "                backbone.layer4[0].downsample[0].stride = (1, 1)\n",
    "            \n",
    "            backbone_modified = nn.Sequential(*list(backbone.children())[:-2])\n",
    "            output_channels = final_channels\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported grid size: {grid_size}. Supported: [7, 14, 28]\")\n",
    "        \n",
    "        print(f\"Backbone {backbone_name} configured for {grid_size}x{grid_size} grid\")\n",
    "        print(f\"Output channels: {output_channels}\")\n",
    "        print(f\"Approximate backbone parameters: {sum(p.numel() for p in backbone_modified.parameters()) / 1e6:.1f}M\")\n",
    "        \n",
    "        return backbone_modified, output_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        adapted_features = self.feature_adapter(features)\n",
    "        predictions = self.prediction_head(adapted_features)\n",
    "        \n",
    "        predictions = predictions.view(\n",
    "            batch_size,\n",
    "            self.num_anchors,\n",
    "            5 + self.num_classes,\n",
    "            predictions.size(-2),\n",
    "            predictions.size(-1)\n",
    "        )\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        backbone_params = sum(p.numel() for p in self.backbone.parameters())\n",
    "        adapter_params = sum(p.numel() for p in self.feature_adapter.parameters())\n",
    "        head_params = sum(p.numel() for p in self.prediction_head.parameters())\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'backbone_parameters': backbone_params,\n",
    "            'adapter_parameters': adapter_params,\n",
    "            'head_parameters': head_params,\n",
    "            'backbone_frozen': not self.backbone[0].weight.requires_grad,\n",
    "            'current_epoch': self.current_epoch\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b22e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_yolo_detector_loss(predictions, targets, coord_weight=5.0, noobj_weight=0.5):\n",
    "    batch_size = predictions.shape[0]\n",
    "    device = predictions.device\n",
    "\n",
    "    pred_xy = torch.sigmoid(predictions[:, :, 0:2])\n",
    "    pred_wh = predictions[:, :, 2:4]            \n",
    "    pred_obj = predictions[:, :, 4:5]             \n",
    "    pred_cls = predictions[:, :, 5:]              \n",
    "\n",
    "    target_xy = targets[:, :, 0:2]\n",
    "    target_wh = targets[:, :, 2:4]\n",
    "    target_obj = targets[:, :, 4:5]\n",
    "    target_cls = targets[:, :, 5:]\n",
    "\n",
    "    obj_mask = target_obj > 0  \n",
    "    noobj_mask = ~obj_mask\n",
    "\n",
    "    obj_mask_expanded = obj_mask.expand_as(pred_xy)  \n",
    "\n",
    "    if obj_mask.sum() > 0:\n",
    "        xy_loss = F.mse_loss(\n",
    "            pred_xy[obj_mask_expanded], \n",
    "            target_xy[obj_mask_expanded], \n",
    "            reduction='sum'\n",
    "        )\n",
    "\n",
    "        wh_loss = F.mse_loss(\n",
    "            pred_wh[obj_mask_expanded], \n",
    "            target_wh[obj_mask_expanded], \n",
    "            reduction='sum'\n",
    "        )\n",
    "    else:\n",
    "        xy_loss = torch.tensor(0.0, device=device)\n",
    "        wh_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    obj_loss = F.binary_cross_entropy_with_logits(\n",
    "        pred_obj[obj_mask], \n",
    "        target_obj[obj_mask], \n",
    "        reduction='sum'\n",
    "    ) if obj_mask.sum() > 0 else torch.tensor(0.0, device=device)\n",
    "\n",
    "    noobj_loss = F.binary_cross_entropy_with_logits(\n",
    "        pred_obj[noobj_mask], \n",
    "        target_obj[noobj_mask], \n",
    "        reduction='sum'\n",
    "    ) if noobj_mask.sum() > 0 else torch.tensor(0.0, device=device)\n",
    "\n",
    "    if obj_mask.sum() > 0:\n",
    "        obj_mask_cls = obj_mask.expand_as(pred_cls)\n",
    "        cls_loss = F.binary_cross_entropy_with_logits(\n",
    "            pred_cls[obj_mask_cls], \n",
    "            target_cls[obj_mask_cls], \n",
    "            reduction='sum'\n",
    "        )\n",
    "    else:\n",
    "        cls_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    total_loss = (\n",
    "        coord_weight * (xy_loss + wh_loss) + \n",
    "        obj_loss + \n",
    "        noobj_weight * noobj_loss + \n",
    "        cls_loss\n",
    "    ) / batch_size\n",
    "\n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'xy_loss': xy_loss / batch_size,\n",
    "        'wh_loss': wh_loss / batch_size,\n",
    "        'obj_loss': obj_loss / batch_size,\n",
    "        'noobj_loss': noobj_loss / batch_size,\n",
    "        'cls_loss': cls_loss / batch_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b42bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, anchors, train_loader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    model.update_epoch(epoch, partial_unfreeze_layers=2)\n",
    "    \n",
    "    running_losses = {'total': 0.0, 'xy': 0.0, 'wh': 0.0, 'obj': 0.0, 'noobj': 0.0, 'cls': 0.0}\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(pbar):\n",
    "        images = torch.stack(images).to(device)\n",
    "        \n",
    "        yolo_targets = create_yolo_targets(\n",
    "            targets, \n",
    "            anchors\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        losses = resnet_yolo_detector_loss(outputs, yolo_targets)\n",
    "        losses['total_loss'].backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        for key in running_losses:\n",
    "            if key == 'total':\n",
    "                running_losses[key] += losses['total_loss'].item()\n",
    "            else:\n",
    "                loss_key = f'{key}_loss'\n",
    "                if loss_key in losses:\n",
    "                    running_losses[key] += losses[loss_key].item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_loss = running_losses['total'] / (batch_idx + 1)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{current_lr:.2e}'})\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    num_batches = len(train_loader)\n",
    "    avg_losses = {key: running_losses[key] / num_batches for key in running_losses}\n",
    "    \n",
    "    return avg_losses\n",
    "\n",
    "def validate_epoch(model, anchors, val_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    running_losses = {'total': 0.0, 'xy': 0.0, 'wh': 0.0, 'obj': 0.0, 'noobj': 0.0, 'cls': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc='Validation'):\n",
    "            images = torch.stack(images).to(device)\n",
    "            \n",
    "            yolo_targets = create_yolo_targets(\n",
    "                targets, \n",
    "                anchors\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            losses = resnet_yolo_detector_loss(outputs, yolo_targets)\n",
    "            \n",
    "            for key in running_losses:\n",
    "                if key == 'total':\n",
    "                    running_losses[key] += losses['total_loss'].item()\n",
    "                else:\n",
    "                    loss_key = f'{key}_loss'\n",
    "                    if loss_key in losses:\n",
    "                        running_losses[key] += losses[loss_key].item()\n",
    "    \n",
    "    num_batches = len(val_loader)\n",
    "    avg_losses = {key: running_losses[key] / num_batches for key in running_losses}\n",
    "    \n",
    "    return avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2afe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training samples: 4437\n",
      "Validation samples: 783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/tobysmith/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/tobysmith/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tobysmith/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone resnet50 configured for 14x14 grid\n",
      "Output channels: 2048\n",
      "Approximate backbone parameters: 23.5M\n",
      "Model Information:\n",
      "  total_parameters: 46,601,301\n",
      "  trainable_parameters: 23,093,269\n",
      "  backbone_parameters: 23,508,032\n",
      "  adapter_parameters: 13,643,776\n",
      "  head_parameters: 9,449,493\n",
      "  backbone_frozen: 1\n",
      "  current_epoch: 0\n",
      "\n",
      "Epoch 1/25\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 135/278 [01:06<01:10,  2.04it/s, loss=354.9033, lr=3.00e-04]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    137\u001b[39m     plot_losses(train_history, val_history, \n\u001b[32m    138\u001b[39m                 save_path=save_dir / \u001b[33m'\u001b[39m\u001b[33mtraining_curves.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m train_losses = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m val_losses = validate_epoch(model, anchors, val_loader, device)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain - Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mXY: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[33m'\u001b[39m\u001b[33mxy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWH: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[33m'\u001b[39m\u001b[33mwh\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    112\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mObj: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[33m'\u001b[39m\u001b[33mobj\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNoObj: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[33m'\u001b[39m\u001b[33mnoobj\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCls: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[\u001b[33m'\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, anchors, train_loader, optimizer, scheduler, device, epoch)\u001b[39m\n\u001b[32m     11\u001b[39m yolo_targets = create_yolo_targets(\n\u001b[32m     12\u001b[39m     targets, \n\u001b[32m     13\u001b[39m     anchors\n\u001b[32m     14\u001b[39m ).to(device)\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m losses = resnet_yolo_detector_loss(outputs, yolo_targets)\n\u001b[32m     20\u001b[39m losses[\u001b[33m'\u001b[39m\u001b[33mtotal_loss\u001b[39m\u001b[33m'\u001b[39m].backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mResNetYOLODetector.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    151\u001b[39m     batch_size = x.size(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     adapted_features = \u001b[38;5;28mself\u001b[39m.feature_adapter(features)\n\u001b[32m    155\u001b[39m     predictions = \u001b[38;5;28mself\u001b[39m.prediction_head(adapted_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torchvision/models/resnet.py:155\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    152\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m    154\u001b[39m out = \u001b[38;5;28mself\u001b[39m.conv3(out)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    158\u001b[39m     identity = \u001b[38;5;28mself\u001b[39m.downsample(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Desktop/dev/python/ML/DL/HW2/.venv/lib/python3.13/site-packages/torch/nn/modules/batchnorm.py:173\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.track_running_stats:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.momentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[32m    175\u001b[39m             exponential_average_factor = \u001b[32m1.0\u001b[39m / \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_batches_tracked)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, filepath)\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return epoch, loss\n",
    "\n",
    "def plot_losses(train_losses, val_losses, save_path=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    epochs = range(1, len(train_losses['total']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Training and Validation Losses')\n",
    "    \n",
    "    loss_types = ['total', 'xy', 'wh', 'obj', 'noobj', 'cls']\n",
    "    titles = ['Total Loss', 'XY Loss', 'WH Loss', 'Objectness Loss', 'No-Object Loss', 'Classification Loss']\n",
    "    \n",
    "    for i, (loss_type, title) in enumerate(zip(loss_types, titles)):\n",
    "        ax = axes[i//3, i%3]\n",
    "        ax.plot(epochs, train_losses[loss_type], 'b-', label='Train')\n",
    "        ax.plot(epochs, val_losses[loss_type], 'r-', label='Validation')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    from pathlib import Path\n",
    "    \n",
    "    config = {\n",
    "        'experiment_name': 'resnet50_YOLO_improved_regularization_14',\n",
    "        'batch_size': 16,\n",
    "        'num_epochs': 25,\n",
    "        'device': 'mps',\n",
    "        'target_size': (224, 224),\n",
    "        'lr': 3e-4,  \n",
    "        'weight_decay': 1e-2,  \n",
    "        'dropout_rate': 0.15,\n",
    "    }\n",
    "    \n",
    "    print(f\"Using device: {config['device']}\")\n",
    "    device = torch.device(config['device'])\n",
    "    \n",
    "    save_dir = \"checkpoints\" / Path(config['experiment_name'])\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    model = ResNetYOLODetector(\n",
    "        anchor_boxes=anchors, \n",
    "        backbone_name=\"resnet50\", \n",
    "        freeze_backbone_epochs=12,\n",
    "        grid_size=14,\n",
    "        dropout_rate=config['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    info = model.get_model_info()\n",
    "    print(\"Model Information:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['lr'], \n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=config['num_epochs'],\n",
    "        eta_min=config['lr'] * 0.01\n",
    "    )\n",
    "    \n",
    "    train_history = {'total': [], 'xy': [], 'wh': [], 'obj': [], 'noobj': [], 'cls': []}\n",
    "    val_history = {'total': [], 'xy': [], 'wh': [], 'obj': [], 'noobj': [], 'cls': []}\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_losses = train_epoch(model, anchors, train_loader, optimizer, scheduler, device, epoch)\n",
    "        val_losses = validate_epoch(model, anchors, val_loader, device)\n",
    "        \n",
    "        print(f\"Train - Total: {train_losses['total']:.4f}, \"\n",
    "              f\"XY: {train_losses['xy']:.4f}, \"\n",
    "              f\"WH: {train_losses['wh']:.4f}, \"\n",
    "              f\"Obj: {train_losses['obj']:.4f}, \"\n",
    "              f\"NoObj: {train_losses['noobj']:.4f}, \"\n",
    "              f\"Cls: {train_losses['cls']:.4f}\")\n",
    "        \n",
    "        print(f\"Val   - Total: {val_losses['total']:.4f}, \"\n",
    "              f\"XY: {val_losses['xy']:.4f}, \"\n",
    "              f\"WH: {val_losses['wh']:.4f}, \"\n",
    "              f\"Obj: {val_losses['obj']:.4f}, \"\n",
    "              f\"NoObj: {val_losses['noobj']:.4f}, \"\n",
    "              f\"Cls: {val_losses['cls']:.4f}\")\n",
    "        \n",
    "        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        for key in train_history:\n",
    "            train_history[key].append(train_losses[key])\n",
    "            val_history[key].append(val_losses[key])\n",
    "        \n",
    "        if val_losses['total'] < best_val_loss:\n",
    "            best_val_loss = val_losses['total']\n",
    "            save_checkpoint(\n",
    "                model, optimizer, scheduler, epoch, val_losses['total'],\n",
    "                save_dir / 'best_model.pth'\n",
    "            )\n",
    "            print(f\"Saved best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    plot_losses(train_history, val_history, \n",
    "                save_path=save_dir / 'training_curves.png')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yjl2ba4z7fk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_for_inference(model_path, device='mps'):\n",
    "    dummy_anchors = torch.tensor([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]])  \n",
    "    model = ResNetYOLODetector(\n",
    "        anchor_boxes=dummy_anchors,\n",
    "        backbone_name=\"resnet50\", \n",
    "        freeze_backbone_epochs=12,\n",
    "        grid_size=14\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "    print(f\"Model was trained for {checkpoint['epoch']} epochs\")\n",
    "    print(f\"Best validation loss: {checkpoint['loss']:.4f}\")\n",
    "    print(f\"Loaded anchors from model: {model.anchors}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def inference_on_images(model, image_paths, device='mps', conf_threshold=0.5, target_size=(224, 224)):\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    anchors = model.anchors\n",
    "    \n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    class_names = {0: 'dog', 1: 'cat'}\n",
    "    colors = {0: 'red', 1: 'blue'}\n",
    "    \n",
    "    num_images = len(image_paths)\n",
    "    cols = min(3, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    _, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes] if cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, image_path in enumerate(image_paths):\n",
    "            original_image = Image.open(image_path).convert('RGB')\n",
    "            original_size = original_image.size\n",
    "            \n",
    "            input_tensor = transform(original_image).unsqueeze(0).to(device)\n",
    "            \n",
    "            predictions = model(input_tensor)\n",
    "            \n",
    "            detections = decode_yolo_predictions(predictions, anchors, conf_threshold=conf_threshold)\n",
    "            \n",
    "            scale_x = original_size[0] / target_size[0]\n",
    "            scale_y = original_size[1] / target_size[1]\n",
    "            \n",
    "            draw_image = original_image.copy()\n",
    "            draw = ImageDraw.Draw(draw_image)\n",
    "            \n",
    "            detection_count = {'cat': 0, 'dog': 0}\n",
    "            \n",
    "            if len(detections) > 0 and len(detections[0]) > 0:\n",
    "                for detection in detections[0]:\n",
    "                    x1, y1, x2, y2, conf, cls = detection\n",
    "                    \n",
    "                    x1 = int(x1 * scale_x)\n",
    "                    y1 = int(y1 * scale_y)\n",
    "                    x2 = int(x2 * scale_x)\n",
    "                    y2 = int(y2 * scale_y)\n",
    "                    \n",
    "                    class_id = int(cls)\n",
    "                    class_name = class_names[class_id]\n",
    "                    color = colors[class_id]\n",
    "                    detection_count[class_name] += 1\n",
    " \n",
    "                    draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n",
    "                    \n",
    "                    label = f'{class_name}: {conf:.2f}'\n",
    "                    \n",
    "                    try:\n",
    "                        font = ImageFont.truetype(\"/System/Library/Fonts/Arial.ttf\", 16)\n",
    "                    except:\n",
    "                        font = ImageFont.load_default()\n",
    "                    \n",
    "                    bbox = draw.textbbox((0, 0), label, font=font)\n",
    "                    text_width = bbox[2] - bbox[0]\n",
    "                    text_height = bbox[3] - bbox[1]\n",
    "                    \n",
    "                    draw.rectangle([x1, y1-text_height-4, x1+text_width+4, y1], fill=color)\n",
    "                    draw.text((x1+2, y1-text_height-2), label, fill='white', font=font)\n",
    "            \n",
    "            ax = axes[idx] if num_images > 1 else axes[0]\n",
    "            ax.imshow(draw_image)\n",
    "            ax.set_title(f'Image {idx+1}\\nCats: {detection_count[\"cat\"]}, Dogs: {detection_count[\"dog\"]}')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            print(f\"Image {idx+1}: {image_path}\")\n",
    "            print(f\"  Detections: {len(detections[0]) if len(detections) > 0 else 0}\")\n",
    "            print(f\"  Cats: {detection_count['cat']}, Dogs: {detection_count['dog']}\")\n",
    "    \n",
    "    for idx in range(num_images, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Load your trained model (anchors automatically loaded from model)\n",
    "model_path = \"checkpoints/resnet50_YOLO_residual_adapter_14/best_model.pth\"\n",
    "device = 'mps'\n",
    "\n",
    "# Load the model - anchors are now loaded automatically from the saved model\n",
    "# model = load_model_for_inference(model_path, device)\n",
    "\n",
    "# Test on single image\n",
    "# inference_on_images(model, \"path/to/your/test/image.jpg\", device)\n",
    "\n",
    "# Test on multiple images\n",
    "# image_paths = [\"path/to/image1.jpg\", \"path/to/image2.jpg\", \"path/to/image3.jpg\"]\n",
    "# inference_on_images(model, image_paths, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
